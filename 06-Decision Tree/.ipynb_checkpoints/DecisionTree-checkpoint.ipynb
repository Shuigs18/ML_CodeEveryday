{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "import math\n",
    "from math import log\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经验熵\n",
    "def entropy(datasets):\n",
    "    data_length = len(datasets)\n",
    "    label_count = {}\n",
    "    for i in range(data_length):\n",
    "        label = datasets[i][-1]\n",
    "        if label not in label_count:\n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "    entropy = - sum([(p / data_length) * log(p / data_length, 2)\n",
    "                    for p in label_count.values()])\n",
    "    return entropy\n",
    "\n",
    "# 条件经验熵\n",
    "def cond_entropy(datasets, axis=0):\n",
    "    \"\"\"\n",
    "    求数据集datasets中第axis列的条件经验熵\n",
    "    \"\"\"\n",
    "    data_length = len(datasets)\n",
    "    feature_sets = {}\n",
    "    for i in range(data_length):\n",
    "        feature = datasets[i][axis]\n",
    "        if feature not in feature_sets:\n",
    "            feature_sets[feature] = []\n",
    "        feature_sets[feature].append(datasets[i])\n",
    "    cond_entropy = sum([(len(p) / data_length) * entropy(p)\n",
    "                        for p in feature_sets.values()])\n",
    "    return cond_entropy\n",
    "\n",
    "# 信息增益 = 经验熵 - 条件经验熵\n",
    "def info_gain(entropy, cond_entropy):\n",
    "    return entropy - cond_entropy\n",
    "\n",
    "# 利用信息增益选择根节点\n",
    "def info_gain_train(datasets):\n",
    "    data_dim = len(datasets[0]) - 1\n",
    "    ent = entropy(datasets)\n",
    "    info_gain_feature = []\n",
    "    for i in range(data_dim):\n",
    "        i_info_gain = info_gain(ent, cond_entropy(datasets, axis=i))\n",
    "        info_gain_feature.append(i_info_gain)\n",
    "        print('特征{}的信息增益为：{}'.format(i + 1, i_info_gain))\n",
    "    best_feature = max(info_gain_feature)\n",
    "    return '特征{}的信息增益最大，选择根节点特征'.format(info_gain_feature.index(best_feature) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [['青年', '否', '否', '一般', '否'],\n",
    "               ['青年', '否', '否', '好', '否'],\n",
    "               ['青年', '是', '否', '好', '是'],\n",
    "               ['青年', '是', '是', '一般', '是'],\n",
    "               ['青年', '否', '否', '一般', '否'],\n",
    "               ['中年', '否', '否', '一般', '否'],\n",
    "               ['中年', '否', '否', '好', '否'],\n",
    "               ['中年', '是', '是', '好', '是'],\n",
    "               ['中年', '否', '是', '非常好', '是'],\n",
    "               ['中年', '否', '是', '非常好', '是'],\n",
    "               ['老年', '否', '是', '非常好', '是'],\n",
    "               ['老年', '否', '是', '好', '是'],\n",
    "               ['老年', '是', '否', '好', '是'],\n",
    "               ['老年', '是', '否', '非常好', '是'],\n",
    "               ['老年', '否', '否', '一般', '否'],\n",
    "               ]\n",
    "labels = [u'年龄', u'有工作', u'有自己的房子', u'信贷情况', u'类别']\n",
    "train_data = pd.DataFrame(datasets, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征1的信息增益为：0.08300749985576883\n",
      "特征2的信息增益为：0.32365019815155627\n",
      "特征3的信息增益为：0.4199730940219749\n",
      "特征4的信息增益为：0.36298956253708536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'特征3的信息增益最大，选择根节点特征'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gain_train(np.array(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义二叉树\n",
    "# 最后生成的决策树类似这种，{'声音': {'粗': {'头发': {'长': '女', '短': '男'}}, '细': '女'}}\n",
    "class Node:\n",
    "    def __init__(self, root = True, label = None, feature_name = None, feature = None):\n",
    "        self.root = root\n",
    "        self.label = label\n",
    "        self.feature_name = feature_name\n",
    "        self.feature = feature \n",
    "        self.tree = {}\n",
    "        self.result = {\n",
    "            'label': self.label, \n",
    "            'feature_name': self.feature_name,\n",
    "            'tree': self.tree\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):  # __repr__魔法函数 返回一个对象的描述信息，可以直接调用对象，不用通过print(object)\n",
    "        return '{}'.format(self.result)\n",
    "    \n",
    "    def add_node(self, val, node):\n",
    "        self.tree[val] = node\n",
    "    \n",
    "    def predict(self, features):\n",
    "        if self.root is True:\n",
    "            return self.label\n",
    "        return self.tree[features[self.feature]].predict(features)\n",
    "        \n",
    "# 生成决策树\n",
    "class DTree:\n",
    "    def __init__(self, epsilon=0.1):  # 信息增益的阈值\n",
    "        self.epsilon = epsilon\n",
    "        self._tree = {}\n",
    "        \n",
    "    # 定义熵\n",
    "    def calc_entropy(self, datasets):\n",
    "        \"\"\"\n",
    "        dataset: np.array(), 不含列名，包含标签列\n",
    "        \"\"\"\n",
    "        data_length = len(datasets)\n",
    "        label_count = {}\n",
    "        for i in range(data_length):\n",
    "            label = datasets[i][-1]\n",
    "            if label not in label_count:\n",
    "                label_count[label] = 0\n",
    "            label_count[label] += 1\n",
    "        entropy = - sum([(p / data_length) * log(p / data_length, 2)\n",
    "                        for p in label_count.values()])\n",
    "        return entropy\n",
    "        \n",
    "    # 定义经验条件熵\n",
    "    def cond_entropy(self, datasets, axis = 0):\n",
    "        \"\"\"\n",
    "        dataset: np.array(), 不含列名，包含标签列\n",
    "        \"\"\"\n",
    "        data_length = len(datasets)\n",
    "        feature_sets = {}\n",
    "        for i in range(data_length):\n",
    "            feature = datasets[i][axis]\n",
    "            if feature not in feature_sets:\n",
    "                feature_sets[feature] = []\n",
    "            feature_sets[feature].append(datasets[i])\n",
    "        cond_entropy = sum([(len(p) / data_length) * self.calc_entropy(p)\n",
    "                            for p in feature_sets.values()])\n",
    "        return cond_entropy\n",
    "    \n",
    "    # 定义信息增益\n",
    "    def info_gain(self, entropy, cond_entropy):\n",
    "        return entropy - cond_entropy\n",
    "    \n",
    "    \n",
    "    def info_gain_train(self, datasets):\n",
    "        \"\"\"\n",
    "        dataset: np.array(), 不含列名，包含标签列\n",
    "        \"\"\"\n",
    "        data_dim = len(datasets[0]) - 1\n",
    "        entropy = self.calc_entropy(datasets)\n",
    "        best_feature = []\n",
    "        for dim in range(data_dim):\n",
    "            dim_info_gain = self.info_gain(entropy, self.cond_entropy(datasets, axis = dim))\n",
    "            best_feature.append((dim, dim_info_gain))\n",
    "        \n",
    "        best_ = max(best_feature, key=lambda x: x[-1]) # 返回(1, 有工作)\n",
    "        return best_\n",
    "    \n",
    "    def train(self, tain_data):\n",
    "        \"\"\"\n",
    "        input: 数据集D(DataFrame)\n",
    "        output: 决策树 eg. {'声音': {'粗': {'头发': {'长': '女', '短': '男'}}, '细': '女'}}\n",
    "        \"\"\"\n",
    "        _, y_train, features = train_data.iloc[:,:-1], train_data.iloc[:, -1], train_data.columns[:-1]\n",
    "        \n",
    "        # 1. 若D中所有实例都是同一类C_k，则T为单节点，并将C_k作为结点的类标记，返回T\n",
    "        if len(y_train.value_counts()) == 1:\n",
    "            return Node(root = True, label = y_train.iloc[0])\n",
    "        \n",
    "        # 2. 若A为空，则T为单节点树，并将D中实例数最大的类标记C_k为该结点的类，返回T\n",
    "        if len(features) == 0:\n",
    "            return Node(root = True, label = y_train.value_counts()[0]) # value_counts中的ascending=Flase，降序排列\n",
    "        \n",
    "        # 3. 计算A中对D的信息增益，选择最大信息增益的Ag\n",
    "        max_feature, max_info_gain = self.info_gain_train(np.array(train_data))\n",
    "        max_feature_name = features[max_feature]\n",
    "        \n",
    "        # 4. Ag的信息增益小于阈值epsilon，则设置T为单节点，并将D中的实例数最大的C_k作为该节点的标记，返回T\n",
    "        if max_info_gain < self.epsilon:\n",
    "            return Node(root = True, label = y_train.value_counts()[0])\n",
    "        \n",
    "        # 5. 否则，对Ag中的每一个a_i，依据Ag=a_i将D分割为若干非空子集D_i，并将D_i中实例数最大的C_k作为该节点的标记，构建子集\n",
    "        node_tree = Node(root=False, feature_name = max_feature_name, feature = max_feature)\n",
    "        feature_list = train_data[max_feature_name].value_counts().index\n",
    "        \n",
    "        for f in feature_list:\n",
    "            sub_train_df = train_data.loc[train_data[max_feature_name] == f].drop([max_feature_name], axis = 1)\n",
    "            # 6. 递归的生成树\n",
    "            sub_tree = self.train(sub_train_df)\n",
    "            node_tree.add_node(f, sub_tree)\n",
    "        \n",
    "        return node_tree\n",
    "    \n",
    "    def fit(self, train_data):\n",
    "        self._tree = self.train(train_data)\n",
    "        return self._tree # self._tree 就是node\n",
    "    \n",
    "    def predict(sel, X_test):\n",
    "        \"\"\"\n",
    "        X_test：输入的单个实例\n",
    "        \"\"\"\n",
    "        return self._tree.predict(X_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c8bb37a3aabf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-263e9d1c14f0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;31m# self._tree 就是node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-263e9d1c14f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, tain_data)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0msub_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_feature_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# 6. 递归的生成树\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0msub_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mnode_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-17-263e9d1c14f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, tain_data)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0msub_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_feature_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_feature_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# 6. 递归的生成树\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0msub_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mnode_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "data_df = pd.DataFrame(datasets, columns=labels)\n",
    "dt = DTree()\n",
    "tree = dt.fit(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
