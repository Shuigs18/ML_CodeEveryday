{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# author: Shuigs18\n",
    "# date: 2021-04-07\n",
    "\n",
    "# 基于tensorflow的多层感知机的实现\n",
    "# 三层（输入、隐藏、输出）MLP + K-fold + weight decay(L2正则化) + dropout\n",
    "# Relu(隐藏) + softmax(输出)\n",
    "# 数据集：fashion—mnist\n",
    "# 梯度计算利用tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据并处理（测试集验证集）\n",
    "# 定义模型参数\n",
    "# 定义激活函数Relu和softmax\n",
    "# 定义网络（dropout实现）\n",
    "# 定义损失函数（加L2正则项 weight decay实现）\n",
    "# K-fold 函数 \n",
    "# 先 k-fold 确定训练集和验证集\n",
    "# 然后在将训练集生成SGD训练的迭代器\n",
    "# train函数 (输入包含训练集迭代器和验证集)\n",
    "#   如果没有传入优化器，定义SGD小批量梯度下降\n",
    "# 返回 fold 0,1,2,3,4,5 训练集验证集的误差\n",
    "# predict函数 生成结果\n",
    "# score函数，计算分数\n",
    "\n",
    "# 数据处理\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "batch_size = 256\n",
    "X_train = tf.cast(X_train, tf.float32)\n",
    "X_test = tf.cast(X_test, tf.float32)\n",
    "X_train = X_train / 255.0   # 颜色的深浅没有关系\n",
    "X_test = X_test / 255.0\n",
    "# 划分批次 \n",
    "# train_iter = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数 两层\n",
    "dim_inputs, dim_hiddens, dim_outputs = 784, 256, 10\n",
    "W1 = tf.Variable(tf.random.normal(shape=(dim_inputs, dim_hiddens), mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "b1 = tf.Variable(tf.zeros(dim_hiddens, dtype=tf.float32))\n",
    "W2 = tf.Variable(tf.random.normal(shape=(dim_hiddens, dim_outputs), mean=0.0, stddev=0.01, dtype=tf.float32))\n",
    "b2 = tf.Variable(tf.random.normal([dim_outputs], mean=0.0, stddev=0.01, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义激活函数 ReLu softmax\n",
    "def ReLu(X):\n",
    "    return tf.math.maximum(X, 0)\n",
    "\n",
    "def softmax(X):\n",
    "    return tf.exp(X) / tf.reduce_sum(tf.math.exp(X), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout(H, drop_prob)\n",
    "# 网络net()\n",
    "def dropout(H, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1 \n",
    "    keep_prob = 1- drop_prob\n",
    "    if keep_prob == 0:\n",
    "        return tf.zeros_like(H)\n",
    "    mask = tf.random.uniform(shape=H.shape, minval=0, maxval=1) < keep_prob\n",
    "    return tf.cast(mask, dtype=tf.float32) * tf.cast(H, dtype=tf.float32) / keep_prob\n",
    "\n",
    "# 定义整个网络\n",
    "drop_prob1 = 0.2\n",
    "def net(X, training=False):\n",
    "    X = tf.reshape(X, shape=(-1, dim_inputs))\n",
    "    H1 = ReLu(tf.matmul(X, W1) + b1)\n",
    "    if training:\n",
    "        H1 = drop_out(H, drop_prob1)\n",
    "    return softmax(tf.matmul(H1, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数 交叉熵 L2正则项\n",
    "def loss_cross_entropy(y_true, y_hat):\n",
    "    return tf.losses.sparse_categorical_crossentropy(y_true, y_hat)\n",
    "\n",
    "def L2_penalty(W):\n",
    "    return tf.reduce_sum(W ** 2) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义K-fold函数\n",
    "def get_K_fold_data(k, i, X, Y):\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, Y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, Y_part = X[idx, :], Y[idx]\n",
    "        if j == i:\n",
    "            X_valid, Y_valid = X_part, Y_part\n",
    "        elif X_train is None:\n",
    "            X_train, Y_train = X_part, Y_part\n",
    "        else:\n",
    "            X_train = tf.concat([X_train, X_part], axis=0)\n",
    "            Y_train = tf.concat([Y_train, Y_part], axis=0)\n",
    "    return X_train, Y_train, X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(net, train_iter, X_valid, Y_valid, loss, num_epochs, batch_size, params=None, learning_rate=None): \n",
    "    train_loss_sum, valid_loss_sum = 0.0, 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, valid_loss, n = 0.0, 0.0, 0\n",
    "        for X_train, Y_train in train_iter:\n",
    "            with tf.GradientTape() as tape:  \n",
    "                Y_hat = net(X_train)\n",
    "                l = loss(Y_train, Y_hat)\n",
    "            # 计算梯度\n",
    "            grads = tape.gradient(l, params)\n",
    "            # 创建一个优化器\n",
    "            opt = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n",
    "            # 梯度下降更新参数\n",
    "            opt.apply_gradients(zip([grad / batch_size for grad in grads], params))\n",
    "            # 更新训练集损失值\n",
    "            train_loss += l.numpy().sum()\n",
    "            n += Y_train.shape[0]\n",
    "        valid_loss += (loss(Y_valid, net(X_valid)).numpy().sum() / Y_valid.shape[0])\n",
    "        train_loss /= n\n",
    "        train_loss_sum += train_loss\n",
    "        valid_loss_sum += valid_loss\n",
    "    train_loss_sum /= num_epochs\n",
    "    valid_loss_sum /= num_epochs\n",
    "    \n",
    "    return params, train_loss_sum, valid_loss_sum\n",
    "\n",
    "def k_fold(k, net, X_train, Y_train, num_epochs, \n",
    "           batch_size, loss_cross_entropy, params=None, learning_rate=None):\n",
    "    start_time = time.time() \n",
    "    for i in range(k):\n",
    "        data = get_K_fold_data(k, i, X_train, Y_train)\n",
    "        train_iter = tf.data.Dataset.from_tensor_slices((data[0], data[1])).batch(batch_size)\n",
    "        X_valid = data[2]\n",
    "        Y_valid = data[3]\n",
    "        params, train_loss, valid_loss =  train(net, train_iter, X_valid, Y_valid, loss_cross_entropy, num_epochs, batch_size, params, learning_rate)\n",
    "        print(\"fold %d: train loss %f, valid loss %f\" % (i, train_loss, valid_loss))\n",
    "    end_time = time.time()\n",
    "    print('总用时：%f' % (start_time - end_time))\n",
    "    return params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数 predict()\n",
    "def predict(net, params, X_test):\n",
    "    Y_hat = net(X_test)\n",
    "    result = tf.argmax(Y_hat, axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0: train loss 0.169400, valid loss 0.169741\n",
      "fold 1: train loss 0.156253, valid loss 0.167162\n",
      "fold 2: train loss 0.147749, valid loss 0.159257\n",
      "fold 3: train loss 0.139382, valid loss 0.157192\n",
      "fold 4: train loss 0.130207, valid loss 0.149647\n",
      "总用时：-114.377255\n"
     ]
    }
   ],
   "source": [
    "params = [W1, b1, W2, b2]\n",
    "num_epochs=10\n",
    "params = k_fold(5, net, X_train, Y_train, num_epochs, batch_size, loss_cross_entropy, params, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(net, params, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
       "array([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 5, 3, 4, 1, 2, 2, 8, 0, 2, 5,\n",
       "       7, 5, 1, 4, 6, 0, 9, 6, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 0, 1, 6, 7,\n",
       "       6, 7, 2, 1, 2, 6, 4, 2, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5, 1, 1,\n",
       "       3, 3, 7, 8, 7, 0, 2, 6, 2, 3, 1, 2, 8, 4, 1, 8, 5, 9, 5, 0, 3, 2,\n",
       "       0, 2, 5, 3, 6, 7, 1, 8, 0, 1, 2, 2])>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
